**`transformer_2`** is a implementation of multi-head attention using
tensorflow 2.0. Notably, it attempts to do *incremental decoding* with
`tf.function`.

Currently this repository is in a highly developmental stage. This README
contains some of the thought process when writing this repository and
highlights some of the present problems.

It should go without saying that this is not a tutorial (and won't be until
the major issues with the implementation are solved). Please forgive the
present heaviness of text.

It will be assumed that the user has a decent understanding of the workings of
multi-head attention, beam search, bottlenecks in deep learning inference, and
`tf.function`.

## Value of *incremental decoding*

The transformer models can be rather slow at inference.<br>
The size of the model combined with the need to run the model for each token
generated adds up to increase the inference duration.<br>
Fortunately there are space for optimization.

For example, the encoder only has to be ran once and the hidden states
generated by the decoder can be cached for future use. We refer to this
procedure as *incremental decoding*.

Facebook AI Research released a version of *incremental decoding* written with
[pytorch](https://pytorch.org) called
[fairseq](https://github.com/pytorch/fairseq) and achieved performance of
~2x inference speed to
[tensor2tensor](https://github.com/tensorflow/tensor2tensor).

If you are a member of FAIR, give yourself a pat on the back because you have
make one heck of a difficult benchmark to beat.

## Problem to address

While fairseq is capable of performing incremental decoding, there still exists
the bottleneck caused by beam search.

At each step of beam search a series of operations (some which involves
CPU to GPU communifcation) has to be conducted to determine the new beam order,
check for completed sentences, and determine the right time to stop inference
early.

Based on tests, it appears that the encoder and decoder forward operations
accounts for only between 25% to 50% of total inference time for fairseq's
incremental decoding. The remainder of the time are spent on beam search
operations which theoretically should be much less computationaly expensive.

Note, if you think 25% to 50% is vague, its because the ratio of time spent
has a huge variance and it is diffiult to give a good value. Will plot a
distribution curve with box plot soonish in the future.

This repository aims to address the issue of slow beam search with
`tf.function` autograph to encapsulate conditional arguments as part of a
graph procedure. This would push the capabilities of tensorflow 2.0 autograph
feature to the extreme.

## Current progress

- [x] Simple training script created
- [x] A dummy inference script is created (for benchmark purposes)
- [ ] Improve dummy inference script performance to match fairseq
- [ ] Improve dummy inference script to do inference at `beam_size > 1`

## Current problems

 - Inference speed is slower than fairseq ლ(ಠ_ಠლ).
