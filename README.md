**`transformer_2`** is a implementation of multi-head attention using
tensorflow 2.0. Notably, it attempts to do *incremental decoding* with
`tf.function`.

Currently this repository is in a highly developmental stage. This README
contains some of the thought process when writing the code and and
highlights some of the problems faced.

It should go without saying that this is not a tutorial (and won't be until
the major issues with the implementation are solved).

It will be assumed that the user has a decent understanding of the workings of
multi-head attention, beam search, various bottlenecks in deep learning
inference, and `tf.function`.

## Value of *incremental decoding*

Autoregressive machine translation can be rather slow at times
since each token has to be generated sequentially.
Re-running the model from start to finish for every word in a
sequence leads to some redundancy in computation.

For example, the encoder only has to be ran once and the hidden states
generated by the decoder can be cached for future use. We refer to this
procedure as *incremental decoding*.

Facebook AI Research released a version of *incremental decoding* written in
[pytorch](https://pytorch.org) called
[fairseq](https://github.com/pytorch/fairseq) and achieved performance of
~2x inference speed to
[tensor2tensor](https://github.com/tensorflow/tensor2tensor).

If you are a member of FAIR, give yourself a pat on the back because you have
make one heck of a difficult benchmark to beat.

## Problem to address

While fairseq is capable of performing incremental decoding, there still exists
the bottlenecks caused by beam search.

At each step of beam search a series of operations (some of which involves
CPU to GPU communifcation) has to be conducted to determine the new beam order,
check for completed sentences, and determine the right time to stop inference
early.

Based on tests, it appears that the encoder and decoder forward operations
accounts for only between 25% to 50% of total inference time for fairseq's
incremental decoding. The remainder of the time are spent on beam search
operations which theoretically should be much less computationaly expensive.

This repository aims to address the issue of slow beam search with
`tf.function` autograph to encapsulate conditional arguments as part of a
graph procedure. Conversion into graph would allow us to improve inference
performance using
[generic graph based optimization](https://www.tensorflow.org/guide/graph_optimization).
This would push the capabilities of tensorflow 2 autograph
feature to the extreme.

## Current progress

- [x] Simple training script created
- [x] A developmental inference script is created (for greedy decoding)
- [ ] Match performance of greedy decoding with that of fairseq.
- [ ] Implement efficient beam search

## Current problems

- Inference speed is slower than fairseq ლ(ಠ_ಠლ).
  - GPU Utilization is at 100%
  - Based on tf.profiler, it apperas some control flow based graph optimization isn't performed
