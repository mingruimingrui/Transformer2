**`transformer_2`** is a implementation of multi-head attention using
tensorflow 2.0. Notably, it attempts to do *incremental decoding* with
`tf.function`.

Currently this repository is in a highly developmental stage. This README
contains some of the thought process when writing this repository and
highlights some of the present problems.

It should go without saying that this is not a tutorial (and won't be until
the major issues with the implementation are solved). Please forgive the
present heaviness of text.

It will be assumed that the user has a decent understanding of the workings of
multi-head attention, beam search, bottlenecks in deep learning inference, and
`tf.function`.

## Value of *incremental decoding*

The transformer models can be rather slow at inference.<br>
The size of the model combined with the need to run the model for each token
generated adds up to increase the inference duration.<br>
Fortunately there are space for optimization.

For example, the encoder only has to be ran once and the hidden states
generated by the decoder can be cached for future use. We refer to this
procedure as *incremental decoding*.

Facebook AI Research released a version of *incremental decoding* written with
[pytorch](https://pytorch.org) called
[fairseq](https://github.com/pytorch/fairseq) and achieved performance of
~2x inference speed to
[tensor2tensor](https://github.com/tensorflow/tensor2tensor).

If you are a member of FAIR, give yourself a pat on the back because you have
make one heck of a difficult benchmark to beat.

## Problem to address

While [fairseq](https://github.com/pytorch/fairseq) is capable of performing
incremental decoding, there still exists the problem of the bottleneck of
CPU to GPU communication.

At each step of beam search a series of CPU-GPU I/O has to be conducted to
determine the new beam order, check for completed sentences, and determine
the right time to stop inference early.

This repository aims to address the issue of CPU-GPU I/O with `tf.function`
autograph to encapsulate conditional arguments as part of a graph procedure.
This would push the capabilities of tensorflow 2.0 autograph feature to the
extreme.

## Current progress

- [x] Simple training script created
- [x] A dummy inference script is created (for benchmark purposes)
- [ ] Improve dummy inference script performance to match fairseq
- [ ] Improve dummy inference script to do inference at `beam_size > 1`

## Current problems

 - Inference speed is slower than [fairseq](https://github.com/pytorch/fairseq)
(TâŒ“T).
